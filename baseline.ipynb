{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/BooksDataset.csv\")\n",
    "df_clean = pd.read_csv(\"Datasets/BooksDatasetClean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns, df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf = df_clean.copy()\n",
    "wdf[\"Description\"] = wdf[\"Description\"].fillna(wdf[\"Category\"])\n",
    "wdf[\"Description\"] = wdf[\"Description\"].fillna(wdf[\"Title\"])\n",
    "wdf[\"Category\"] = wdf[\"Category\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "wdf['Category_list'] = wdf['Category'].str.split(' , ')\n",
    "wdf['Category_list'] = wdf['Category_list'].apply(lambda arr: [s.strip() for s in arr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_categories = mlb.fit_transform(wdf['Category_list'])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_categories, columns=mlb.classes_)\n",
    "\n",
    "wdf = pd.concat([wdf, encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Set a random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def product_text_embedding(text):\n",
    "    encoding = tokenizer.batch_encode_plus( [text],\n",
    "        padding=True,              \n",
    "        truncation=True,           \n",
    "        return_tensors='pt',      \n",
    "        add_special_tokens=True    \n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'] \n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state \n",
    "\n",
    "    sentence_embedding = word_embeddings.mean(dim=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "# Input text\n",
    "text = f\"peepeepoopoo\"\n",
    "\n",
    "\n",
    "product_text_embedding(text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = wdf.copy()\n",
    "\n",
    "exp_df = exp_df.drop(columns=['Authors', 'Category', 'Category_list', 'Publisher', 'Price Starting With ($)', 'Publish Date (Month)', 'Publish Date (Year)'])\n",
    "\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def create_new_dataframe(df):\n",
    "  new_df = pd.DataFrame()\n",
    "  numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "  length = df.shape[0]\n",
    "  for index, row in df.iterrows():\n",
    "    vector = product_text_embedding(row['Description']).reshape(768)\n",
    "    numerical_values = row[numerical_cols].values\n",
    "    name = row['Title']  \n",
    "\n",
    "    combined_vector = np.concatenate((vector, numerical_values))  \n",
    "\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({'book_embedding': [combined_vector], \n",
    "                                            'name': [name]})], ignore_index=True)\n",
    "    \n",
    "    print(index/length, end='\\r')\n",
    "\n",
    "  return new_df\n",
    "\n",
    "def find_closest_record(record, new_df):\n",
    "  record_vector = record['book_embedding']\n",
    "\n",
    "  closest_name = None\n",
    "  min_distance = float('inf')\n",
    "\n",
    "  for index, row in new_df.iterrows():\n",
    "    other_vector = row['book_embedding']\n",
    "\n",
    "    print(record_vector.shape, other_vector.shape)\n",
    "    # Calculate cosine similarity for combined vectors\n",
    "    similarity = cosine_similarity(record_vector.reshape(1, -1), other_vector.reshape(1, -1))[0][0]\n",
    "\n",
    "    distance = 1 - similarity\n",
    "\n",
    "    if distance < min_distance:\n",
    "      min_distance = distance\n",
    "      closest_name = row['Title']\n",
    "\n",
    "  return closest_name\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming you have a DataFrame called 'df' and a function 'f' that generates vectors\n",
    "# new_df = create_new_dataframe(df, f)\n",
    "\n",
    "# Example record from the new DataFrame\n",
    "# record = new_df.iloc[0]\n",
    "\n",
    "# Find the closest record\n",
    "# closest_record_name = find_closest_record(record, new_df)\n",
    "# print(f\"The closest record to '{record['name']}' is '{closest_record_name}'.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = create_new_dataframe(exp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_record(new_df.iloc[0], new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
